#!/usr/bin/env python3
"""Cloud video → diarized transcription via Gemini.

Downloads video files from Google Drive or Dropbox, extracts audio with
ffmpeg, transcribes with speaker diarization via Gemini 2.5 Pro, and
uploads transcripts + mp3s back to the source cloud.

Usage:
    transcribe_drive list <folder_url_or_id> [--source {drive,dropbox}]
    transcribe_drive transcribe <file_id> [--output-folder ID] [--source {drive,dropbox}]
    transcribe_drive batch <folder_url_or_id> [--output-folder ID] [--budget $] [--source {drive,dropbox}]
"""
import sys
import os
import subprocess
import shutil
import time
import re
import argparse
import json
from datetime import datetime, timezone
from pathlib import Path

# --- VIRTUAL ENVIRONMENT BOOTSTRAP ---
script_path = Path(__file__).resolve()
script_dir = script_path.parent
script_stem = script_path.stem
venv_dir = script_dir / f".{script_stem}_venv"
venv_python = venv_dir / "bin" / "python"

REQUIREMENTS = [
    "google-genai",
    "google-auth",
    "google-auth-oauthlib",
    "google-api-python-client",
    "python-dotenv",
    "dropbox",
]

if not venv_dir.exists():
    print(f"Creating virtual environment at {venv_dir}...")
    subprocess.run([sys.executable, "-m", "venv", str(venv_dir)], check=True)
    subprocess.run(
        [str(venv_python), "-m", "pip", "install", "-U", "pip"] + REQUIREMENTS,
        check=True,
    )

if sys.executable != str(venv_python):
    os.execv(str(venv_python), [str(venv_python)] + sys.argv)

# --- IMPORTS AFTER BOOTSTRAP ---
from dotenv import load_dotenv
from google import genai

# Make providers/ importable
sys.path.insert(0, str(script_dir))
from providers import detect_provider, get_provider

load_dotenv(script_dir / "transcribe.env")

# --- CONSTANTS ---
GEMINI_MODEL = "gemini-2.5-pro"
# Gemini 2.5 Pro pricing (per 1M tokens)
PRICE_INPUT_PER_M = 1.25
PRICE_OUTPUT_PER_M = 10.00
# Rough estimate: ~32 tokens/sec of audio for Gemini
TOKENS_PER_SEC = 32

DIARIZATION_PROMPT = """\
Transcribe this audio with speaker diarization.

Rules:
- Identify each unique speaker and label them consistently (Speaker 1, Speaker 2, etc.).
- If you can identify a speaker by name from context clues (e.g. someone addresses them \
by name, or they introduce themselves), include their name in parentheses after the \
speaker label, like: Speaker 1 (Tim Draper):
- Once you identify a speaker's name, use it consistently for all their subsequent turns.
- Include timestamps at the start of each speaker turn in [MM:SS] format, relative to \
the start of the audio.
- Output as plain text, one speaker turn per line.
- Do not omit any speech. Transcribe everything spoken.
- If multiple people talk at once, transcribe the dominant speaker and note crosstalk in \
parentheses if significant.

Format each line as:
[MM:SS] Speaker N (Name if known): What they said...
"""


# --- GENERIC HELPERS (cloud-agnostic) ---

def get_gemini_client():
    """Build a Gemini API client."""
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("ERROR: GEMINI_API_KEY not set. Check ~/bin/transcribe.env")
        sys.exit(1)
    return genai.Client(
        api_key=api_key,
        http_options={"timeout": 600_000},  # 10 min timeout for long transcriptions
    )


def extract_audio(video_path: Path, audio_path: Path) -> None:
    """Extract audio track from video using ffmpeg."""
    print(f"  Extracting audio → {audio_path.name}...")
    result = subprocess.run(
        [
            "ffmpeg", "-i", str(video_path),
            "-vn",                     # no video
            "-acodec", "libmp3lame",   # mp3 codec
            "-q:a", "2",               # high quality VBR
            "-y",                      # overwrite
            str(audio_path),
        ],
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        print(f"  ffmpeg error:\n{result.stderr}")
        raise RuntimeError("ffmpeg audio extraction failed")
    size_mb = audio_path.stat().st_size / 1e6
    print(f"  Audio extracted: {size_mb:.1f} MB")


def get_audio_duration(audio_path: Path) -> float:
    """Get audio duration in seconds using ffprobe."""
    result = subprocess.run(
        [
            "ffprobe", "-v", "quiet",
            "-show_entries", "format=duration",
            "-of", "default=noprint_wrappers=1:nokey=1",
            str(audio_path),
        ],
        capture_output=True,
        text=True,
    )
    try:
        return float(result.stdout.strip())
    except ValueError:
        return 0.0


def upload_to_gemini(client, audio_path: Path):
    """Upload audio to Gemini Files API and wait for ACTIVE."""
    print(f"  Uploading to Gemini Files API...")
    uploaded = client.files.upload(file=audio_path)
    print(f"  Uploaded: {uploaded.name} ({uploaded.size_bytes} bytes)")

    while uploaded.state.name == "PROCESSING":
        print("  Waiting for Gemini to process file...", end="\r")
        time.sleep(3)
        uploaded = client.files.get(name=uploaded.name)

    if uploaded.state.name == "FAILED":
        raise RuntimeError(f"Gemini file processing failed: {uploaded.state}")

    print(f"  Gemini file ready: {uploaded.state.name}")
    return uploaded


def transcribe_with_gemini(client, uploaded_file) -> tuple[str, dict]:
    """Run diarized transcription. Returns (transcript_text, usage_dict)."""
    print(f"  Requesting diarized transcription...")
    response = client.models.generate_content(
        model=GEMINI_MODEL,
        contents=[uploaded_file, DIARIZATION_PROMPT],
    )
    usage = {}
    if response.usage_metadata:
        usage = {
            "input_tokens": response.usage_metadata.prompt_token_count or 0,
            "output_tokens": response.usage_metadata.candidates_token_count or 0,
        }
    text = response.text
    if not text:
        raise RuntimeError(
            f"Gemini returned empty/None response "
            f"(finish_reason: {response.candidates[0].finish_reason if response.candidates else 'no candidates'})"
        )
    return text, usage


def estimate_cost(duration_secs: float, output_tokens: int = 10000) -> float:
    """Estimate Gemini API cost for an audio file."""
    input_tokens = duration_secs * TOKENS_PER_SEC
    input_cost = (input_tokens / 1e6) * PRICE_INPUT_PER_M
    output_cost = (output_tokens / 1e6) * PRICE_OUTPUT_PER_M
    return input_cost + output_cost


def send_ntfy(title: str, body: str) -> None:
    """Send a push notification via ntfy.sh (reads NTFY_TOPIC / NTFY_EMAIL from env)."""
    import urllib.request
    topic = os.environ.get("NTFY_TOPIC", "")
    if not topic:
        return
    try:
        headers = {"Title": title}
        email = os.environ.get("NTFY_EMAIL", "")
        if email:
            headers["Email"] = email
        req = urllib.request.Request(
            f"https://ntfy.sh/{topic}",
            data=body.encode(),
            headers=headers,
        )
        urllib.request.urlopen(req, timeout=10)
    except Exception as e:
        print(f"  (notification failed: {e})")


def format_duration(seconds: float) -> str:
    """Format seconds as HH:MM:SS."""
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    return f"{m}:{s:02d}"


def format_size(size_bytes: int | str) -> str:
    """Format bytes as human-readable size."""
    n = int(size_bytes)
    for unit in ["B", "KB", "MB", "GB", "TB"]:
        if n < 1024:
            return f"{n:.1f} {unit}"
        n /= 1024
    return f"{n:.1f} PB"


def build_transcript_header(file_meta: dict, duration_secs: float) -> str:
    """Build a metadata header block to prepend to transcripts."""
    now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    lines = [
        f"# Source: {file_meta['name']}",
        f"# Source ID: {file_meta['id']}",
        f"# Source URL: {file_meta.get('webViewLink', 'N/A')}",
        f"# Source Created: {file_meta.get('createdTime', 'N/A')}",
        f"# Source Modified: {file_meta.get('modifiedTime', 'N/A')}",
        f"# Audio Duration: {format_duration(duration_secs)}",
        f"# Transcribed: {now}",
        "# ---",
        "",
    ]
    return "\n".join(lines)


def sanitize_name(name: str) -> str:
    """Turn a folder name into a safe subfolder component."""
    # Replace any character that isn't alphanumeric, dash, underscore, or dot
    safe = re.sub(r"[^\w.\-]", "_", name)
    return safe.strip("_") or "job"


def resolve_provider(args):
    """Determine the source provider from --source flag or auto-detect."""
    source = getattr(args, "source", None)
    if source:
        return get_provider(source, script_dir)
    ref = getattr(args, "folder", None) or getattr(args, "file_id", None) or ""
    return get_provider(detect_provider(ref), script_dir)


def resolve_output_provider(args):
    """Determine the output provider from --output-source flag or auto-detect from --output-folder."""
    output_source = getattr(args, "output_source", None)
    if output_source:
        return get_provider(output_source, script_dir)
    ref = getattr(args, "output_folder", None) or ""
    if not ref:
        return None
    return get_provider(detect_provider(ref), script_dir)


def process_single_file(provider, service, gemini, file_meta: dict,
                        transcript_folder_id: str | None,
                        audio_folder_id: str | None,
                        output_folder_id: str | None,
                        tmp_dir: Path,
                        out_provider=None,
                        out_service=None) -> tuple[dict, float, float, dict]:
    """Process one file: stream/download → transcribe → upload.

    out_provider/out_service are used for uploads; fall back to source
    provider/service if not specified (same-provider case).

    Returns (manifest_entry, actual_cost, duration, usage).
    """
    if out_provider is None:
        out_provider = provider
    if out_service is None:
        out_service = service
    name = file_meta["name"]
    file_id = file_meta["id"]
    base_name = Path(name).stem

    video_path = tmp_dir / name
    audio_path = tmp_dir / f"{base_name}.mp3"
    transcript_path = tmp_dir / f"{base_name}.txt"

    try:
        # Stream or download + extract audio
        try:
            provider.stream_audio(service, file_id, audio_path)
        except (PermissionError, RuntimeError) as stream_err:
            print(f"  Stream failed: {stream_err}")
            print(f"  Falling back to full download...")
            provider.download_file(service, file_id, video_path)
            extract_audio(video_path, audio_path)
            video_path.unlink()

        duration = get_audio_duration(audio_path)
        est_cost = estimate_cost(duration)
        print(f"  Audio duration: {format_duration(duration)}")

        # Sanity check: very short audio from a large file suggests a failed stream
        file_size = file_meta.get("size", 0) or 0
        if file_size > 5 * 1024 ** 3 and duration < 300:  # >5 GB file, <5 min audio
            raise RuntimeError(
                f"Suspiciously short audio ({format_duration(duration)}) from "
                f"{format_size(file_size)} file — stream likely dropped. Re-run to retry."
            )
        print(f"  Estimated cost: ${est_cost:.3f}")

        # Upload to Gemini and transcribe
        uploaded = upload_to_gemini(gemini, audio_path)
        transcript, usage = transcribe_with_gemini(gemini, uploaded)

        try:
            gemini.files.delete(name=uploaded.name)
        except Exception:
            pass

        # Prepend metadata header and save transcript locally
        header = build_transcript_header(file_meta, duration)
        transcript_with_header = header + transcript
        transcript_path.write_text(transcript_with_header, encoding="utf-8")
        print(f"  Transcript: {len(transcript)} chars, {transcript.count(chr(10))+1} lines")

        actual_cost = 0.0
        if usage:
            actual_cost = (
                (usage["input_tokens"] / 1e6) * PRICE_INPUT_PER_M
                + (usage["output_tokens"] / 1e6) * PRICE_OUTPUT_PER_M
            )
            print(f"  Tokens: {usage['input_tokens']:,} in / {usage['output_tokens']:,} out")
            print(f"  Actual cost: ${actual_cost:.3f}")

        # Upload results to output provider (may differ from source provider)
        if transcript_folder_id:
            out_provider.upload_file(out_service, transcript_folder_id, transcript_path, "text/plain")
            print(f"  Uploaded transcript: transcripts/{base_name}.txt")
        if audio_folder_id:
            out_provider.upload_file(out_service, audio_folder_id, audio_path, "audio/mpeg")
            print(f"  Uploaded audio: audio/{base_name}.mp3")

        # Build manifest entry
        now = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
        manifest_entry = {
            "source_file_id": file_id,
            "source_file_name": name,
            "source_url": file_meta.get("webViewLink", ""),
            "source_created": file_meta.get("createdTime", ""),
            "source_modified": file_meta.get("modifiedTime", ""),
            "source_size_bytes": int(file_meta.get("size", 0)),
            "transcript_file": f"transcripts/{base_name}.txt",
            "audio_file": f"audio/{base_name}.mp3",
            "audio_duration_secs": round(duration, 1),
            "transcribed_at": now,
            "gemini_input_tokens": usage.get("input_tokens", 0),
            "gemini_output_tokens": usage.get("output_tokens", 0),
            "gemini_cost_usd": round(actual_cost, 4),
        }

        if not output_folder_id:
            print(f"\n{'='*60}")
            print(transcript_with_header)
            print(f"{'='*60}")

        return manifest_entry, actual_cost, duration, usage

    finally:
        for p in [video_path, audio_path, transcript_path]:
            if p.exists():
                p.unlink()


# --- COMMANDS ---

def cmd_list(args):
    """List video files in a cloud folder."""
    provider = resolve_provider(args)
    service = provider.connect()
    folder_ref = provider.extract_folder_ref(args.folder)
    print(f"Listing video files in folder: {folder_ref}\n")

    files = provider.list_video_files(service, folder_ref)

    if not files:
        print("No video files found.")
        return

    total_size = 0
    for i, f in enumerate(files, 1):
        size = int(f.get("size", 0))
        total_size += size
        created = f.get("createdTime", "")[:10] or "N/A"
        modified = f.get("modifiedTime", "")[:10] or "N/A"
        folder_path = f.get("_folder_path", "")
        display_name = f"{folder_path}{f['name']}" if folder_path else f['name']
        print(f"  {i:3d}. {display_name}")
        print(f"       ID: {f['id']}  Size: {format_size(size)}  Created: {created}  Modified: {modified}")

    print(f"\n{len(files)} files, {format_size(total_size)} total")


def cmd_transcribe(args):
    """Transcribe a single file."""
    provider = resolve_provider(args)
    service = provider.connect()
    out_provider = resolve_output_provider(args)
    out_service = out_provider.connect() if out_provider else None
    gemini = get_gemini_client()

    file_id = args.file_id
    output_folder_id = args.output_folder

    file_meta = provider.get_file_metadata(service, file_id)
    print(f"\nProcessing: {file_meta['name']} ({format_size(file_meta.get('size', 0))})")

    # Create a job-specific output subfolder to avoid conflicts with concurrent jobs
    transcript_folder_id = None
    audio_folder_id = None
    job_folder_id = None
    if output_folder_id and out_provider and out_service:
        src_name = sanitize_name(provider.get_folder_name(service, file_id))
        job_folder_id = out_provider.ensure_subfolder(out_service, output_folder_id, src_name)
        print(f"Output subfolder: {src_name}")
        transcript_folder_id = out_provider.ensure_subfolder(out_service, job_folder_id, "transcripts")
        audio_folder_id = out_provider.ensure_subfolder(out_service, job_folder_id, "audio")

    tmp_dir = Path(f"/tmp/transcribe_drive/{os.getpid()}")
    tmp_dir.mkdir(parents=True, exist_ok=True)

    manifest_entry, actual_cost, duration, usage = process_single_file(
        provider, service, gemini, file_meta,
        transcript_folder_id, audio_folder_id, job_folder_id, tmp_dir,
        out_provider=out_provider, out_service=out_service,
    )

    # Update manifest in job subfolder
    if job_folder_id and out_provider and out_service:
        manifest, manifest_file_id = out_provider.load_manifest(out_service, job_folder_id)
        manifest["files"].append(manifest_entry)
        out_provider.save_manifest(out_service, job_folder_id, manifest, manifest_file_id)
        print(f"  Manifest updated")

    return manifest_entry


BATCH_PIDFILE = Path("/tmp/transcribe_drive_batch.pid")


def cmd_batch(args):
    """Batch-transcribe all video files in a cloud folder."""
    BATCH_PIDFILE.write_text(str(os.getpid()))
    try:
        _cmd_batch(args)
    finally:
        BATCH_PIDFILE.unlink(missing_ok=True)


def _cmd_batch(args):
    """Inner implementation of cmd_batch."""
    provider = resolve_provider(args)
    service = provider.connect()
    out_provider = resolve_output_provider(args)
    out_service = out_provider.connect() if out_provider else None
    gemini = get_gemini_client()

    folder_ref = provider.extract_folder_ref(args.folder)
    output_folder_id = args.output_folder
    budget = args.budget

    print(f"Source folder: {folder_ref}")

    # Create a job-specific output subfolder named after the source folder
    # so concurrent jobs writing to the same output root don't collide.
    job_folder_id = None
    if output_folder_id and out_provider and out_service:
        src_name = sanitize_name(provider.get_folder_name(service, folder_ref))
        job_folder_id = out_provider.ensure_subfolder(out_service, output_folder_id, src_name)
        print(f"Output subfolder: {src_name}/ (in {output_folder_id})")

    # List source files
    files = provider.list_video_files(service, folder_ref)
    if not files:
        print("No video files found.")
        return

    print(f"Found {len(files)} video files")

    # Apply --filter if specified
    if getattr(args, "filter", None):
        pattern = args.filter.lower()
        before = len(files)
        files = [f for f in files if pattern in f["name"].lower()]
        print(f"Filter '{args.filter}': {before} → {len(files)} files")

    # Apply --exclude if specified (repeatable)
    if getattr(args, "exclude", None):
        before = len(files)
        for pattern in args.exclude:
            files = [f for f in files if pattern.lower() not in f["name"].lower()]
        print(f"Exclude {args.exclude}: {before} → {len(files)} files")

    # Load manifest for skip logic (by source file ID)
    manifest = None
    manifest_file_id = None
    transcribed_ids = set()
    transcript_folder_id = None
    audio_folder_id = None
    if job_folder_id and out_provider and out_service:
        transcript_folder_id = out_provider.ensure_subfolder(out_service, job_folder_id, "transcripts")
        audio_folder_id = out_provider.ensure_subfolder(out_service, job_folder_id, "audio")
        manifest, manifest_file_id = out_provider.load_manifest(out_service, job_folder_id)
        manifest["source_folder_id"] = folder_ref
        # Remove --reprocess targets from manifest so they get re-transcribed
        reprocess = getattr(args, "reprocess", None) or []
        if reprocess and manifest.get("files"):
            before = len(manifest["files"])
            manifest["files"] = [
                e for e in manifest["files"]
                if not any(pat.lower() in e.get("name", "").lower() for pat in reprocess)
            ]
            removed = before - len(manifest["files"])
            if removed:
                print(f"  Reprocess: removed {removed} manifest entries matching {reprocess}")
        transcribed_ids = {e["source_file_id"] for e in manifest.get("files", [])}
        if transcribed_ids:
            print(f"  Manifest loaded: {len(transcribed_ids)} files already transcribed")

    # Filter out already-transcribed files by source ID
    pending = []
    for f in files:
        if f["id"] in transcribed_ids:
            print(f"  SKIP (in manifest): {f['name']}")
        else:
            pending.append(f)

    if not pending:
        print("\nAll files already transcribed!")
        return

    print(f"\n{len(pending)} files to transcribe ({len(transcribed_ids)} already done)")

    # Start notification
    est_total = estimate_cost(3600) * len(pending)  # rough: ~1hr/file
    budget_str = f"Budget: ${budget:.2f}" if budget else f"Est. cost: ~${est_total:.2f}"
    total_size = sum(f.get("size", 0) for f in pending)
    send_ntfy(
        "Transcribe batch: STARTED",
        "\n".join([
            f"Files: {len(pending)} to process ({len(transcribed_ids)} already done)",
            f"Size: {format_size(total_size)}",
            budget_str,
        ]),
    )

    # Budget tracking
    total_cost = 0.0
    total_input_tokens = 0
    total_output_tokens = 0
    total_duration = 0.0
    results = []
    budget_limit = budget
    start_time = time.time()

    tmp_dir = Path("/tmp/transcribe_drive")
    tmp_dir.mkdir(parents=True, exist_ok=True)

    for i, f in enumerate(pending, 1):
        name = f["name"]
        print(f"\n{'='*60}")
        print(f"[{i}/{len(pending)}] {name} ({format_size(f.get('size', 0))})")
        print(f"{'='*60}")

        try:
            manifest_entry, file_cost, duration, usage = process_single_file(
                provider, service, gemini, f,
                transcript_folder_id, audio_folder_id, job_folder_id, tmp_dir,
                out_provider=out_provider, out_service=out_service,
            )

            total_cost += file_cost
            total_duration += duration
            if usage:
                total_input_tokens += usage.get("input_tokens", 0)
                total_output_tokens += usage.get("output_tokens", 0)

            # Set budget limit after first file if not specified
            if budget_limit is None and i == 1 and file_cost > 0:
                budget_limit = file_cost * len(pending) * 2
                print(f"  Auto-budget set: ${budget_limit:.2f} "
                      f"(2x extrapolated from first file)")

            # Update manifest via output provider
            if manifest is not None and out_provider and out_service:
                manifest["files"].append(manifest_entry)
                manifest_file_id = out_provider.save_manifest(
                    out_service, job_folder_id, manifest, manifest_file_id
                )
                print(f"  → Manifest updated ({len(manifest['files'])} entries)")

            results.append({
                "name": name,
                "duration": duration,
                "cost": file_cost,
                "input_tokens": usage.get("input_tokens", 0),
                "output_tokens": usage.get("output_tokens", 0),
            })

            print(f"  Cost: ${file_cost:.3f} | Running total: ${total_cost:.2f}")

        except Exception as e:
            print(f"  ERROR processing {name}: {e}")
            results.append({"name": name, "error": str(e)})

        # Budget check (before next file)
        if budget_limit is not None and total_cost >= budget_limit:
            remaining = len(pending) - i
            if remaining > 0:
                print(f"\n  BUDGET LIMIT: Reached ${budget_limit:.2f} "
                      f"(spent ${total_cost:.2f})")
                print(f"  Stopping batch. {remaining} files remaining.")
                break

        # Delay between files to avoid rate limiting
        if i < len(pending):
            delay = int(os.environ.get("TRANSCRIBE_DELAY", "45"))
            print(f"  Waiting {delay}s before next file...")
            time.sleep(delay)

    # --- SUMMARY ---
    elapsed = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"BATCH COMPLETE")
    print(f"{'='*60}")
    successes = [r for r in results if "error" not in r]
    failures = [r for r in results if "error" in r]
    print(f"  Files processed: {len(successes)}/{len(pending)}")
    if failures:
        print(f"  Failures: {len(failures)}")
        for r in failures:
            print(f"    - {r['name']}: {r['error']}")
    print(f"  Total audio: {format_duration(total_duration)}")
    print(f"  Total tokens: {total_input_tokens:,} in / {total_output_tokens:,} out")
    print(f"  Total cost: ${total_cost:.2f}")
    print(f"  Elapsed time: {format_duration(elapsed)}")
    if successes:
        avg_cost = total_cost / len(successes)
        print(f"  Avg cost/file: ${avg_cost:.3f}")
    # End notification
    status = "SUCCESS" if not failures else f"PARTIAL ({len(failures)} failures)"
    if not successes and failures:
        status = "FAILED (all files errored)"
    parts = [
        f"Processed: {len(successes)}/{len(pending)}",
        f"Cost: ${total_cost:.2f}",
        f"Duration: {format_duration(elapsed)}",
    ]
    if failures:
        parts.append(f"Failed: {', '.join(r['name'] for r in failures)}")
    send_ntfy(f"Transcribe batch: {status}", "\n".join(parts))

    # Clean up pid-specific tmp dir
    shutil.rmtree(tmp_dir, ignore_errors=True)


# --- MAIN ---

def main():
    parser = argparse.ArgumentParser(
        description="Cloud video → diarized transcription via Gemini",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    subparsers = parser.add_subparsers(dest="command", required=True)

    source_help = "Source provider: drive or dropbox (default: auto-detect from URL)"
    output_source_help = "Output provider: drive or dropbox (default: auto-detect from --output-folder)"

    # list
    p_list = subparsers.add_parser("list", help="List video files in a folder")
    p_list.add_argument("folder", help="Folder URL, ID, or Dropbox path")
    p_list.add_argument("--source", choices=["drive", "dropbox"], default=None,
                        help=source_help)

    # transcribe
    p_trans = subparsers.add_parser("transcribe", help="Transcribe a single file")
    p_trans.add_argument("file_id", help="File ID (Drive) or path (Dropbox)")
    p_trans.add_argument("--output-folder", help="Output folder ID or path")
    p_trans.add_argument("--source", choices=["drive", "dropbox"], default=None,
                         help=source_help)
    p_trans.add_argument("--output-source", choices=["drive", "dropbox"], default=None,
                         help=output_source_help)

    # batch
    p_batch = subparsers.add_parser("batch", help="Batch-transcribe all files in a folder")
    p_batch.add_argument("folder", help="Folder URL, ID, or Dropbox path")
    p_batch.add_argument("--output-folder", help="Output folder ID or path")
    p_batch.add_argument("--budget", type=float, default=None,
                         help="Max spend in dollars (default: 2x estimated)")
    p_batch.add_argument("--source", choices=["drive", "dropbox"], default=None,
                         help=source_help)
    p_batch.add_argument("--output-source", choices=["drive", "dropbox"], default=None,
                         help=output_source_help)
    p_batch.add_argument("--filter", default=None,
                         help="Only process files whose name contains this string (case-insensitive)")
    p_batch.add_argument("--exclude", action="append", default=None, metavar="PATTERN",
                         help="Skip files whose name contains this string (case-insensitive, repeatable)")
    p_batch.add_argument("--reprocess", action="append", default=None, metavar="PATTERN",
                         help="Remove matching files from manifest and re-transcribe them (repeatable)")

    args = parser.parse_args()

    if args.command == "list":
        cmd_list(args)
    elif args.command == "transcribe":
        cmd_transcribe(args)
    elif args.command == "batch":
        cmd_batch(args)


if __name__ == "__main__":
    main()
